
# ğŸ“˜ Multiple Linear Regression â€“ Complete Notes

## ğŸ”¹ Introduction
Multiple Linear Regression (MLR) is an extension of Simple Linear Regression. It models the relationship between a dependent variable (Y) and two or more independent variables (Xâ‚, Xâ‚‚, ..., Xâ‚™).

It assumes a linear relationship between the predictors and the response variable.

## ğŸ”¹ What is Multiple Linear Regression?
- A supervised learning regression technique.
- Target (output) variable Y is continuous.
- Used when we want to predict Y based on multiple features.
- For example:
  - Predicting house price based on area, location, number of bedrooms, age, etc.
  - Predicting salary based on education level, years of experience, skill set, etc.

## ğŸ”¹ Mathematical Model (Equation)
**Y = bâ‚€ + bâ‚Xâ‚ + bâ‚‚Xâ‚‚ + â‹¯ + bâ‚™Xâ‚™ + Îµ**

Where:
- Y: Dependent variable (response)
- Xâ‚...Xâ‚™: Independent variables (features)
- bâ‚€: Intercept (constant term)
- bâ‚...bâ‚™: Coefficients for each predictor
- Îµ: Error term (residuals)

Each báµ¢ represents the effect of one unit change in Xáµ¢, holding other variables constant.

## ğŸ”¹ Objective
Estimate the best-fitting coefficients bâ‚€, bâ‚, ..., bâ‚™ to minimize the prediction error.

Use Least Squares Method to minimize:

**MSE = (1/n) Ã— âˆ‘(Yáµ¢ - Å¶áµ¢)Â²**

## ğŸ”¹ Assumptions of Multiple Linear Regression
- Linearity â€“ Relationship between predictors and response is linear.
- Independence â€“ Observations are independent of each other.
- Homoscedasticity â€“ Constant variance of residuals (errors).
- No multicollinearity â€“ Predictors are not highly correlated with each other.
- Normality â€“ Residuals are normally distributed.

## ğŸ”¹ Matrix Representation
Y = XÎ² + Îµ

- Y: nÃ—1 vector of target values
- X: nÃ—(p+1) matrix with a column of 1s for intercept
- Î²: (p+1)Ã—1 vector of coefficients
- Îµ: error vector

## ğŸ”¹ Solving for Coefficients â€“ Closed-Form Solution
**Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€Y**

## ğŸ”¹ Python Implementation

```python
# Step 1: Import Libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Step 2: Dataset
X = np.array([[1, 2], [2, 3], [4, 5], [3, 6], [5, 8]])
Y = np.array([10, 12, 20, 18, 26])

# Step 3: Train Model
model = LinearRegression()
model.fit(X, Y)
print("Intercept:", model.intercept_)
print("Coefficients:", model.coef_)

# Step 4: Predictions
Y_pred = model.predict(X)
print("Predictions:", Y_pred)
```

## ğŸ”¹ Interpreting Coefficients
- bâ‚: change in Y for one unit increase in Xâ‚, holding Xâ‚‚ constant.
- bâ‚‚: change in Y for one unit increase in Xâ‚‚, holding Xâ‚ constant.

## ğŸ”¹ Model Evaluation Metrics
- MSE  = (1/n) Ã— âˆ‘(yáµ¢ - Å·áµ¢)Â² â†’ Mean squared error
- RMSE = âˆšMSE â†’ Root mean squared error
- MAE  = (1/n) Ã— âˆ‘|yáµ¢ - Å·áµ¢| â†’ Mean absolute error
- RÂ²   = 1 - (SSR/SST) â†’ Goodness of fit

## ğŸ”¹ Pros and Cons

### Advantages:
- Easy to interpret.
- Efficient for small to medium-sized datasets.
- Works well when assumptions are met.

### Limitations:
- Sensitive to multicollinearity.
- Can underperform on nonlinear data.
- Assumes linear relationships.

## ğŸ”¹ When to Use MLR
- You have multiple features affecting the target.
- The relationship is expected to be linear.
- You need a baseline model.

## ğŸ”¹ Visual Example
MLR with 2 features forms a plane in 3D; in higher dimensions, it becomes a hyperplane.

## âœ… Summary
- Model: Y = bâ‚€ + bâ‚Xâ‚ + bâ‚‚Xâ‚‚ + ... + bâ‚™Xâ‚™ + Îµ
- Goal: Minimize MSE
- Technique: Least Squares (OLS)
- Assumptions: Linearity, Independence, Homoscedasticity, No Multicollinearity, Normality
- Evaluation: MSE, RMSE, MAE, RÂ²
- Python: sklearn.linear_model.LinearRegression
