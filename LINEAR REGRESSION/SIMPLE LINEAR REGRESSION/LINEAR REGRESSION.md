# üìò Simple Linear Regression ‚Äì Complete Notes

## üîπ Introduction to Regression

Regression is a **supervised learning** technique used to model the **relationship between input features (independent variables)** and a **continuous output variable (dependent variable)**.

### What is Regression?

- It‚Äôs a **predictive modeling technique**.
- Used when the **target variable is continuous** (e.g., house price, temperature, salary).
- Unlike classification, which predicts **categories**, regression predicts **quantitative values**.

### Types of Regression

1. **Simple Linear Regression**:  
   - Only one independent variable (X) and one dependent variable (Y).  
   - Equation of a line is used to model the relationship.

2. **Multiple Linear Regression**:  
   - Involves two or more independent variables (X‚ÇÅ, X‚ÇÇ, ..., X‚Çô).
   - The model becomes:
     \[
     y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n
     \]

---

## üîπ Simple Linear Regression (SLR)

Simple Linear Regression finds the best-fitting straight line to describe the relationship between a **single input feature (X)** and a **target variable (Y)**.

### The SLR Model:

\[
\hat{y} = mx + b
\]

Where:

- \( \hat{y} \) is the **predicted value** of the target variable  
- \( m \) is the **slope** (rate of change)  
- \( x \) is the input variable  
- \( b \) is the **intercept** (value of \( y \) when \( x = 0 \))

---

### Objective of Linear Regression

- Find the **best values** for `m` and `b` such that the line **minimizes prediction error**.
- The most common error metric is **Mean Squared Error (MSE)**:
  \[
  MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
  \]
- The goal is to find `m` and `b` that minimize this error.

---

## üîπ Code Example (Implementation using Python)

Here‚Äôs how we can implement Simple Linear Regression step-by-step:

### 1. Import Required Libraries
```python
import numpy as np
import matplotlib.pyplot as plt
```

### 2. Sample Dataset
```python
# Input (X) and output (Y) data
X = np.array([1, 2, 3, 4, 5])
Y = np.array([2, 4, 5, 4, 5])
```

### 3. Define Prediction Function
```python
def predict(x, m, b):
    return m * x + b
```

### 4. Calculate Slope (m) and Intercept (b)
```python
# Number of samples
n = len(X)

# Mean of X and Y
mean_x = np.mean(X)
mean_y = np.mean(Y)

# Numerator and denominator for slope
numerator = np.sum((X - mean_x) * (Y - mean_y))
denominator = np.sum((X - mean_x) ** 2)

# Calculate slope (m) and intercept (b)
m = numerator / denominator
b = mean_y - m * mean_x
```

### 5. Predictions and Plot
```python
# Generate predicted Y values
Y_pred = predict(X, m, b)

# Plotting the results
plt.scatter(X, Y, color='blue', label='Actual Data')
plt.plot(X, Y_pred, color='red', label='Regression Line')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Simple Linear Regression')
plt.legend()
plt.grid(True)
plt.show()
```

---

## üîπ Intuition Behind Simple Linear Regression

Imagine you have a scatterplot of points.

### What are we trying to do?

- Draw a straight line through the points so that the **overall distance from each point to the line is minimized**.
- These distances are called **residuals** (errors between actual and predicted values).

Why a straight line?

- The assumption is: there's a **linear relationship** between `X` and `Y`.
- That means when X increases or decreases, Y also tends to increase or decrease proportionally.

The best-fitting line is the one that **minimizes the sum of squared residuals**.

---

## üîπ How to Find Slope (m) and Intercept (b)

To find the line of best fit, we calculate the slope `m` and intercept `b` using the **least squares method**:

### Slope Formula (m):
\[
m = \frac{\sum{(x_i - \bar{x})(y_i - \bar{y})}}{\sum{(x_i - \bar{x})^2}}
\]

- Numerator: covariance between X and Y  
- Denominator: variance of X

### Intercept Formula (b):
\[
b = \bar{y} - m\bar{x}
\]

- Where:
  - \( \bar{x} \) = mean of input feature X  
  - \( \bar{y} \) = mean of target variable Y

These formulas ensure that the fitted line has the **minimum possible MSE**.

---

### üîç Alternate Explanation

Imagine fitting a line where the **total vertical distance between actual points and the line is minimized**. You:
- Use means of `x` and `y` to center your data.
- Calculate how much `y` changes with a unit change in `x` (this is slope).
- Then shift the line up/down (intercept) so that it aligns properly with data.

This is the core idea behind **Ordinary Least Squares (OLS)** method.

---

## ‚úÖ Summary

- Simple Linear Regression predicts continuous output based on a single input feature.
- The equation is:  
  \[
  \hat{y} = mx + b
  \]
- The slope `m` shows how much `y` changes with a unit increase in `x`.
- The intercept `b` is the predicted value of `y` when `x = 0`.
- The objective is to **minimize Mean Squared Error (MSE)**.
- Parameters are found using **Least Squares method**.
